<h1>Quantifying uncertainty</h1>

<p>
    When developing decision making or decision support systems, we would usually like to build trust in the system - both for the developer, the end user, and those who need to approve use.
</p>

<p>
    The progression of trust usually goes like this:
    <ol>
        <li>We establish a baseline or formalize the existing process which already has a certain level of trust.</li>
        <li>We demonstrate an improvement over the baseline at the level of model or process quality statistics.</li>
        <li>We demonstrate improvement over the baseline at the level of process testing.</li>
    </ol>
</p>

<p>
    If you find yourself in a case where you are able to demonstrate a comprehensive improvement over the baseline or existing process, then the situation is probably pretty simple - just switch to the new system, it's always better[1]. More likely though, something like this is true - your new system is usually better (better on average) but you can find cases where the old process was better[2]. In these more nuanced cases, it would be very useful to know when you could trust each system.
</p>

<p>
    When I was a kid, it was very common to ask someone what time it is[3]. This question came with two typical responses:

    <ul>
        <li>The correct time.</li>
        <li>The response "I don't know" or equivalent.</li>
    </ul>

    This ability to gauge uncertainty is crucial in our ability to learn from one another and as a community. Indeed, it is very rare to find one person who knows more or is better at making decisions in every case - instead we have a variety of opinions and we are able to assign or solicit signals as to how much we should trust each position.
</p>

<p>
    In particular, it would be really bad if the person we asked for the time responded confidently with an incorrect time they made up.[4] In any case, (suffienciently motivated by the above discussion) we turn now to different methodologies for quantifying uncertainty.
</p>

<h2>Bayes rule</h2>

<p>
    Bayes' rule gives us a helpful way to estimate a posterior distribution in the presence of a prior belief and a likelihood. Let's define these concepts with some suggestive and motivated language you likely won't find in a probability textbook:
</p>

<ul>
    <li>Posterior distribution: What we think about a parameter in the presence of data.</li>
    <li>Prior distribution: What we thought about that parameter before we had any data.</li>
    <li>Likelihood: How likely the data we have is assuming our parameter beliefs are true.</li>
</ul>

<p>
    We can derive Bayes' rule from the definition of conditional probability:
</p>



<p>
    P(H | X) * P(X) = P(X &cap; H) = P(X | H) * P(H)
</p>

<p>
    Solving for one conditional probability, we get the usual formulation of Bayes' rule:
</p>

<p>
    P(X | H) = P(H | X) * P(X) / P(H)
</p>

<p>
    Which we might say succintly in words as: "Posterior is proportional to prior times likelihood."
</p>

<p>
    Let's see what kind of calculation this enables now. Specifically, if we fix some data H and a prior distribution P(X), can we calculate P(X | H)?
</p>

<p>
    Since H is fixed, whatever P(H) is, it's a constant (the right constant to make our P(X | H) integrate to 1 as all probability distributions should) so we can ignore it for now.
</p>

<p>
    This means, our ability to calculate the posterior comes down to our ability to calculate the likelihood function P(X | H).
</p>

<p>
    Like all good students of probablity, let start with an easy case of flipping coins.
</p>

<h3>Beta distributions and bernouilli likelihoods</h3>


<p>
    Suppose we have a mysterious coin given to us by a magician. On one side of the coin is printed "heads" and on the other is printed "tails". We intend to use this coin for various games of chance and would like it to be fair. That said, this magician is quite mysterious and we have no clue how this coin is going to behave. Formalizing our thoughts a bit, we can say that there is some rate at which coin flips will come up "heads" - we call this rate the "heads rate" and denote it with X.
</p>

<p>
    Since we have no idea what p might be, we will use a uniform distribution to describe our prior beliefs. That is,
</p>

<p>
    P(X) = 1 for 0 &leq; X &leq; 1
    P(X) = 0 otherwise 
</p>

<p>
    If we want to have an informed belief about the true heads rate of this mysterious coin, we should go about generating data. So, let's say that we flip the coin 10 times and come up with the following history: {T, H, T, T, T, T, H, H, T, T}. In all, 3 heads and 7 tails.
</p>

<p>
    Given a true heads rate of p, what is the likelihood of seeing this data? Well, it would be: (1-p)^7 * (p^3).
</p>



<h2>Null hypothesis testing</h2>

<h2>Credible intervals</h2>

<h2>Confidence intervals in linear regression</h2>

<h2>Bootstrapping</h2>

<p>
    [1] It's maybe a bit more complicated than this - maybe the old process is cheaper, faster, better understood (does not require new training), or produces useful artifacts related to other processes. Or maybe the switching costs are not compensated by the increase in efficiency.
</p>

<p>
    [2] For example, maybe the two processes in question really come down to a model. One model may be better than the other in aggregate (higher AUC) but you will almost certainly find cases where individual predictions are better on the other model.
</p>

<p>
    [3] I don't know if this is still a common question or not, but it's believable that knowledge of the current time and date has become more widespread.
</p>

<p>
    [4] I suspect this is one important reason why most people don't like liars or other people who confidently tell us things that aren't true.
</p>

